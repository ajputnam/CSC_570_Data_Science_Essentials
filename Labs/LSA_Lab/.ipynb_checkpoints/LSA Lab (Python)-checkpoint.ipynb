{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis Lab\n",
    "**University of Illinois**\n",
    "<br>CSC 570 - Data Science Essentials\n",
    "<br>Author: Arthur Putnam\n",
    "\n",
    "## Lab Directions\n",
    "Your assignment for this week is to do LSA on a group of newsgroup posts from the newsgroup 'rec.sport.baseball.'  (Feel free to pick another newsgroup if you like, the list is here.  http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)   \n",
    "\n",
    "1.  To get the newsgroup data, use this code:\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups<br>\n",
    "categories = ['rec.sport.baseball']<br>\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories)<br>\n",
    "corpus = dataset.data<br>\n",
    "\n",
    "2.  Next, you'll be adapting my LSA code for your problem.  This shouldn't be too hard, but please spend some time understanding what my code is doing.  \n",
    "\n",
    "3.  When you print the discovered concepts you'll probably find they don't make sense.  Consider adjusting the words in the stop word list to remove things like nntp, and people's names...\n",
    "\n",
    "4.  Once youre satisfied with your work, submit the link to your work\n",
    "\n",
    "## Libraries \n",
    "* NLTK - Natural Language Toolkit [http://www.nltk.org/](http://www.nltk.org/)\n",
    "* sklearn - Python Machine Learning kit [http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)\n",
    "\n",
    "## Terms\n",
    "Latent Semantic Analysis - is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. [Wikipedia](https://en.wikipedia.org/wiki/Latent_semantic_analysis)\n",
    "\n",
    "corpus - a collection of written texts, especially the entire works of a particular author or a body of writing on a particular subject.[merriam-webster](https://www.merriam-webster.com/dictionary/corpus)\n",
    "\n",
    "## Data Set\n",
    "[The 20 newsgroups text dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set loaded!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['rec.sport.baseball']\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True, \n",
    "                             random_state=42, \n",
    "                             categories=categories, \n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "corpus = dataset.data\n",
    "print (\"Data set loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "There is some data preparation we want to do.\n",
    "* Lowercase\n",
    "* Remove names\n",
    "* Remove emails\n",
    "* Common works like: and, the, or, of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aj-\n",
      "[nltk_data]     Pu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package names to C:\\Users\\Aj-\n",
      "[nltk_data]     Pu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "The data has been lowercased and emails, punctuation, numbers, and stopwords have been removed.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import names\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "nltk.download('stopwords')\n",
    "nltk.download('names')\n",
    "\n",
    "# a set of words to 'ignore' \n",
    "stopset = set()\n",
    "# add common words from the english language \n",
    "stopset.update(stopwords.words('english'))\n",
    "# add common names to the stopset (lowercased)\n",
    "stopset.update([name.lower() for name in names.words()])\n",
    "# words that were found to have little meaning\n",
    "stopset.update(['from', 'to', 're', 'subject', 'dl', 'i think', 'dont', 'th', 'maybe', 'gilkey', 'yorku', \n",
    "                'Alomar', 'alomar', 'baerga', 'Baerga', 'kubey', 'Kubey', 'kirsch', 'Kirsch', 'Traven', \n",
    "                'traven', 'koufax', 'would', 'think', 'list', 'thanks', 'mailing','mailing list','please',\n",
    "                'anyone','email','mail','send','please email','extra', 'dcon','dops','nhs','contribution',\n",
    "                'compared','hes','pm','am', 'net', 'com','md','hp','hewlettpackard', 'animal', 'beyond', \n",
    "                'natural', 'yall', 'chop', 'spanishspeaking', 'although', 'internet', 'comes', 'tanstaafl',\n",
    "                'something', 'like', 'rf','era','cf','lf','bb', 'idle', 'hs', 'formula', 'thought', 'ap',\n",
    "                'also', 'read', 'able', 'much', 'humor', 'ss', 'ab', 'rbi', 'bchmbiochemdukeedu', 'ls', 'gif','oh well',\n",
    "               'neb', 'anyway', 'want', 'find', 'ny', 'widespread', 'fla', 'spanish', 'could', 'wife', 'young', 'name'])\n",
    "\n",
    "def remove_emails(text):\n",
    "    \"\"\" Uses regex to remove email addresses from text\"\"\"\n",
    "    return re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return ''.join([l for l in text if not l.isdigit()])\n",
    "\n",
    "def lowercase(text):\n",
    "    \"\"\" Lowercases the text \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join(l for l in text if l not in string.punctuation)\n",
    "\n",
    "def remove_stopset_words(text):\n",
    "    querywords = text.split()\n",
    "    resultwords  = [word for word in querywords if word.lower() not in stopset]\n",
    "    result = ' '.join(resultwords)\n",
    "    return result\n",
    "\n",
    "def clean_text(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_emails(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopset_words(text)\n",
    "    text = remove_numbers(text)\n",
    "    return text\n",
    "    \n",
    "cleaned_corpus = [clean_text(text) for text in corpus]\n",
    "print('The data has been lowercased and emails, punctuation, numbers, and stopwords have been removed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data before being cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hear ya!  Then again, we must remember that we are indeed Cub fans, and\n",
      "that the Cubs will eventually blow it.  After all, the Cubs are the easiest\n",
      "team in the National League to root for.  No Pressure.  You know they will\n",
      "lose eventually.  Oh well, I suppose we must have faith.  After all, they\n",
      "do look pretty good, and they don't even have Sandberg back yet.  \n",
      "\n",
      "CUBS IN '93!!!!!\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data after being cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hear ya must remember indeed cub fans cubs eventually blow cubs easiest team national league root pressure know lose eventually oh well suppose must look pretty good even sandberg back yet cubs \n"
     ]
    }
   ],
   "source": [
    "print(cleaned_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopset, use_idf=True, ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 994\n",
      "Number of terms: 100222\n"
     ]
    }
   ],
   "source": [
    "X.shape\n",
    "print('Number of documents:', X.shape[0])\n",
    "print('Number of terms:', X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x100222 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 36857)\t0.0805087888282\n",
      "  (0, 98197)\t0.094599979551\n",
      "  (0, 54914)\t0.133840048217\n",
      "  (0, 71580)\t0.0621540871968\n",
      "  (0, 41704)\t0.0889356534341\n",
      "  (0, 18381)\t0.094599979551\n",
      "  (0, 27063)\t0.0610192531333\n",
      "  (0, 18421)\t0.193076812856\n",
      "  (0, 25255)\t0.184958756475\n",
      "  (0, 9820)\t0.0906088753149\n",
      "  (0, 23172)\t0.10805584877\n",
      "  (0, 86366)\t0.0455113827919\n",
      "  (0, 55205)\t0.0704637489118\n",
      "  (0, 46478)\t0.0504275562406\n",
      "  (0, 73399)\t0.0970480341075\n",
      "  (0, 67134)\t0.0906088753149\n",
      "  (0, 44858)\t0.0452049889868\n",
      "  (0, 49503)\t0.0757082905978\n",
      "  (0, 58308)\t0.0699750666738\n",
      "  (0, 95310)\t0.0443197550349\n",
      "  (0, 85122)\t0.0835921648883\n",
      "  (0, 49009)\t0.0582613171425\n",
      "  (0, 67160)\t0.0575854313267\n",
      "  (0, 34222)\t0.0438484765461\n",
      "  (0, 24947)\t0.0502876353387\n",
      "  :\t:\n",
      "  (0, 18385)\t0.114495007563\n",
      "  (0, 27091)\t0.114495007563\n",
      "  (0, 18449)\t0.114495007563\n",
      "  (0, 25257)\t0.114495007563\n",
      "  (0, 9824)\t0.114495007563\n",
      "  (0, 18447)\t0.114495007563\n",
      "  (0, 23176)\t0.114495007563\n",
      "  (0, 86641)\t0.114495007563\n",
      "  (0, 55237)\t0.114495007563\n",
      "  (0, 46695)\t0.114495007563\n",
      "  (0, 73409)\t0.114495007563\n",
      "  (0, 67138)\t0.114495007563\n",
      "  (0, 45017)\t0.114495007563\n",
      "  (0, 49517)\t0.114495007563\n",
      "  (0, 25265)\t0.114495007563\n",
      "  (0, 58358)\t0.114495007563\n",
      "  (0, 95653)\t0.114495007563\n",
      "  (0, 85137)\t0.114495007563\n",
      "  (0, 54962)\t0.114495007563\n",
      "  (0, 49124)\t0.114495007563\n",
      "  (0, 67215)\t0.114495007563\n",
      "  (0, 34335)\t0.114495007563\n",
      "  (0, 25154)\t0.114495007563\n",
      "  (0, 75169)\t0.114495007563\n",
      "  (0, 5464)\t0.114495007563\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=30, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=30, n_iter=100)\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "lost\n",
      "year\n",
      "game\n",
      "team\n",
      "last\n",
      "games\n",
      "one\n",
      "good\n",
      "hit\n",
      "sox\n",
      " \n",
      "Concept 1:\n",
      "lost\n",
      "new york\n",
      "york\n",
      "chicago\n",
      "san\n",
      "new\n",
      "american\n",
      "angels\n",
      "national\n",
      "sox\n",
      " \n",
      "Concept 2:\n",
      "hits\n",
      "average\n",
      "stolen\n",
      "hits stolen\n",
      "defensive\n",
      "fielder\n",
      "bases\n",
      "outs\n",
      "prevented\n",
      "average fielder\n",
      " \n",
      "Concept 3:\n",
      "year\n",
      "last year\n",
      "clutch\n",
      "last\n",
      "better\n",
      "good\n",
      "team\n",
      "years\n",
      "average\n",
      "players\n",
      " \n",
      "Concept 4:\n",
      "team\n",
      "games\n",
      "runs\n",
      "pitching\n",
      "jays\n",
      "sox\n",
      "last year\n",
      "toronto\n",
      "staff\n",
      "last\n",
      " \n",
      "Concept 5:\n",
      "stats\n",
      "baseball\n",
      "players\n",
      "station\n",
      "know\n",
      "sox\n",
      "jewish\n",
      "heard\n",
      "local\n",
      "day\n",
      " \n",
      "Concept 6:\n",
      "sox\n",
      "year\n",
      "station\n",
      "last\n",
      "last year\n",
      "hit\n",
      "heard sox\n",
      "heard sox wrol\n",
      "know scmets\n",
      "know scmets yankmes\n",
      " \n",
      "Concept 7:\n",
      "runs\n",
      "clutch\n",
      "sox\n",
      "stats\n",
      "hit\n",
      "station\n",
      "batting\n",
      "run\n",
      "home\n",
      "pitcher\n",
      " \n",
      "Concept 8:\n",
      "jewish\n",
      "kingman jewish\n",
      "kingman\n",
      "tune\n",
      "fingers\n",
      "staub\n",
      "pitcher\n",
      "article\n",
      "blomberg\n",
      "greenberg\n",
      " \n",
      "Concept 9:\n",
      "stats\n",
      "clutch\n",
      "tune\n",
      "kids\n",
      "games\n",
      "people\n",
      "stick\n",
      "stadium\n",
      "giants\n",
      "game\n",
      " \n",
      "Concept 10:\n",
      "jewish\n",
      "games\n",
      "game\n",
      "field\n",
      "stick\n",
      "clutch\n",
      "year\n",
      "stats\n",
      "trash\n",
      "baseball\n",
      " \n",
      "Concept 11:\n",
      "year\n",
      "university\n",
      "last year\n",
      "stats\n",
      "last\n",
      "pitcher\n",
      "baseball\n",
      "rule\n",
      "colorado boulder\n",
      "university colorado\n",
      " \n",
      "Concept 12:\n",
      "university\n",
      "team\n",
      "braves\n",
      "colorado boulder\n",
      "university colorado\n",
      "university colorado boulder\n",
      "boulder\n",
      "colorado\n",
      "means\n",
      "baseball tend\n",
      " \n",
      "Concept 13:\n",
      "games\n",
      "clutch\n",
      "university\n",
      "many\n",
      "colorado boulder\n",
      "university colorado\n",
      "university colorado boulder\n",
      "boulder\n",
      "hit\n",
      "boston\n",
      " \n",
      "Concept 14:\n",
      "clutch\n",
      "gant\n",
      "tune\n",
      "year\n",
      "hirschbeck\n",
      "runs\n",
      "game\n",
      "come\n",
      "umpire\n",
      "last year\n",
      " \n",
      "Concept 15:\n",
      "university\n",
      "colorado boulder\n",
      "university colorado\n",
      "university colorado boulder\n",
      "boulder\n",
      "beloved\n",
      "colorado\n",
      "swallows\n",
      "means\n",
      "baseball tend\n",
      " \n",
      "Concept 16:\n",
      "stadium\n",
      "come\n",
      "beloved\n",
      "swallows\n",
      "day\n",
      "tigers\n",
      "central\n",
      "hanshin\n",
      "hanshin tigers\n",
      "yakult\n",
      " \n",
      "Concept 17:\n",
      "beloved\n",
      "jewish\n",
      "team\n",
      "swallows\n",
      "clutch\n",
      "hanshin\n",
      "hanshin tigers\n",
      "yakult\n",
      "central\n",
      "tigers\n",
      " \n",
      "Concept 18:\n",
      "bonds\n",
      "williams\n",
      "pitchers\n",
      "team\n",
      "pitches\n",
      "batting\n",
      "get\n",
      "aldred neid\n",
      "aldred neid parrett\n",
      "benavides\n",
      " \n",
      "Concept 19:\n",
      "stats\n",
      "vizquel\n",
      "riles\n",
      "braves\n",
      "beloved\n",
      "barehanded\n",
      "ball\n",
      "swallows\n",
      "fielded\n",
      "career\n",
      " \n",
      "Concept 20:\n",
      "stadium\n",
      "bichette\n",
      "aldred neid\n",
      "aldred neid parrett\n",
      "benavides\n",
      "benavides pitchers\n",
      "benavides pitchers totals\n",
      "bichette girardi\n",
      "bichette girardi castilla\n",
      "castilla\n",
      " \n",
      "Concept 21:\n",
      "ball\n",
      "rule\n",
      "stadium\n",
      "fly\n",
      "infield fly\n",
      "back\n",
      "runner\n",
      "university\n",
      "maine\n",
      "penobscot\n",
      " \n",
      "Concept 22:\n",
      "stadium\n",
      "gant\n",
      "hirschbeck\n",
      "bonds\n",
      "williams\n",
      "id stadium\n",
      "team\n",
      "id\n",
      "strike\n",
      "runs\n",
      " \n",
      "Concept 23:\n",
      "mattingly\n",
      "history\n",
      "first\n",
      "best\n",
      "baseman\n",
      "first baseman\n",
      "baseballalways\n",
      "ever\n",
      "one\n",
      "ever nohitters\n",
      " \n",
      "Concept 24:\n",
      "sox\n",
      "games\n",
      "team\n",
      "stadium\n",
      "first\n",
      "stats\n",
      "hit\n",
      "curran\n",
      "curran data\n",
      "curran data central\n",
      " \n",
      "Concept 25:\n",
      "baseball\n",
      "fly\n",
      "scores\n",
      "first\n",
      "ball\n",
      "infield fly\n",
      "braves\n",
      "runner\n",
      "mattingly\n",
      "hernandez\n",
      " \n",
      "Concept 26:\n",
      "maine\n",
      "penobscot\n",
      "penobscot university\n",
      "penobscot university maine\n",
      "robbins penobscot\n",
      "robbins penobscot university\n",
      "university maine\n",
      "robbins\n",
      "university\n",
      "mattingly\n",
      " \n",
      "Concept 27:\n",
      "hit\n",
      "games\n",
      "ever\n",
      "cubs\n",
      "team\n",
      "couple\n",
      "one\n",
      "kingman\n",
      "history\n",
      "field\n",
      " \n",
      "Concept 28:\n",
      "grafitti\n",
      "grafitti paris\n",
      "grafitti paris close\n",
      "groucho\n",
      "groucho sort\n",
      "groucho sort grafitti\n",
      "marxist\n",
      "marxist groucho\n",
      "marxist groucho sort\n",
      "paris\n",
      " \n",
      "Concept 29:\n",
      "first\n",
      "claim\n",
      "said\n",
      "cubs\n",
      "sure\n",
      "buschs\n",
      "buschs judgement\n",
      "buschs judgement left\n",
      "claim impeachable\n",
      "claim impeachable source\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print(\"Concept %d:\" % i )\n",
    "    for term in sortedTerms:\n",
    "        print(term[0])\n",
    "    print (\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
